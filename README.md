# Real-Time Twitter Data Pipeline

## ðŸ“Š Overview
This project builds a real-time data pipeline to stream tweets from Twitter, push them to Kafka, process them using PySpark, and load them into BigQuery for SQL-based analytics.

## ðŸ§° Architecture
```
Twitter API --> Kafka (Docker) --> PySpark (Structured Streaming) --> BigQuery (GCP)
```

## ðŸ“„ Folder Structure
```
real-time-twitter-pipeline/
â”œâ”€â”€ docker/                # Kafka + Zookeeper setup
â”œâ”€â”€ ingestion/             # Twitter to Kafka producer
â”œâ”€â”€ processing/            # PySpark to BigQuery consumer
â”œâ”€â”€ config.json            # Configs for Spark & BigQuery
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env.example           # Template for secrets
â”œâ”€â”€ .gitignore             # Ignore sensitive/local files
â””â”€â”€ README.md              # Project guide
```

## ðŸ“– How to Run

### 1. Start Kafka
```bash
cd docker
docker-compose up
```

### 2. Start Tweet Ingestion
Edit `twitter_producer.py` and paste your Twitter Bearer Token:
```python
BEARER_TOKEN = "your_actual_token"
```
Then run:
```bash
python ingestion/twitter_producer.py
```

### 3. Start Spark Stream
Edit `config.json` and update:
- `gcp_project`
- `bigquery_dataset`
- `bigquery_table`
- `gcs_temp_bucket`

Then run:
```bash
spark-submit processing/spark_stream.py
```

## ðŸŽ¯ Output
Tweets with `id` and `text` fields will be stored in your BigQuery table in real time.
